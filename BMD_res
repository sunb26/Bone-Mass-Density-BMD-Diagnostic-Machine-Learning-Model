{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BMD_res","provenance":[],"authorship_tag":"ABX9TyNNbAiCAB2ZJFeyKh0poUWQ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"mU1C1ei2A_UT"},"source":["import pandas as pd\n","import numpy as np\n","from imblearn.over_sampling import SMOTE\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression, SGDClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","from sklearn import preprocessing\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_WPnWc9xDVPg"},"source":["column_names = ['Diagnosis','Age','Sex','L1','L2','L3','L4', 'R1','R2', 'R3','Lf1','Lf2','Lf3']\n","\n","raw_dataset = pd.read_csv(\"training.csv\", names=column_names, na_values = \"?\", comment='\\t', sep=\",\", skipinitialspace=True)\n","target = 'Diagnosis'\n","X = raw_dataset.loc[:, raw_dataset.columns!= target]\n","Y = raw_dataset.loc[:, raw_dataset.columns==target]\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Ccr4rRFDbne"},"source":["X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size =0.15, random_state=0)\n","\n","\n","#Creating two copies of the dataset and using one-vs-rest to create binary sets for SMOTE oversampling\n","\n","Y1_train = Y_train.copy()\n","Y2_train = Y_train.copy()\n","Y3_train = Y_train.copy()\n","Y1_test = Y_test.copy()\n","Y2_test = Y_test.copy()\n","Y3_test = Y_test.copy()\n","\n","\n","\n","y1_new = Y1_train.replace(2,0)\n","y2_new = Y2_train.replace(1,0)\n","y2_new = y2_new.replace(2,1)\n","y3_new= Y3_train.replace(0,4)\n","y3_new= y3_new.replace(1,0)\n","y3_new= y3_new.replace(2,0)\n","y3_new = y3_new.replace(4,1)\n","\n","y1_test_new = Y1_test.replace(2,0)\n","y2_test_new = Y2_test.replace(1,0)\n","y2_test_new = y2_test_new.replace(2,1)\n","y3_test_new= Y3_test.replace(0,4)\n","y3_test_new= y3_test_new.replace(1,0)\n","y3_test_new= y3_test_new.replace(2,0)\n","y3_test_new = y3_test_new.replace(4,1)\n","\n","\n","\n","#One-Hot Encoding\n","#train_labels = Y_train.pop('Diagnosis')\n","#test_labels = Y_test.pop('Diagnosis')\n","#onehot_encoder = OneHotEncoder(sparse = False)\n","#label_encoder = LabelEncoder()\n","#int_encoder = label_encoder.fit_transform(train_labels)\n","#train_data = int_encoder.reshape(len(train_labels),1)\n","#onehot_encoded = onehot_encoder.fit_transform(train_data) \n","#Y_train = pd.DataFrame(onehot_encoded)\n","#int_encoder = label_encoder.fit_transform(test_labels)\n","#test_data = int_encoder.reshape(len(test_labels),1)\n","#onehot_encoded = onehot_encoder.fit_transform(test_data) \n","#Y_test = pd.DataFrame(onehot_encoded)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zTSDqbUsktmJ"},"source":["##x1 = ['apple', 'banana']\n","#x2 = ['pineapple', 'sandwhich']\n","#y1 = [1,2,3]\n","#y2 = [4,5,6]\n","\n","#df1 = pd.DataFrame(x1, columns=['C1'])\n","#df2 = pd.DataFrame(x2, columns=['C2'])\n","#df3 = pd.DataFrame(y1, columns=['C1'])\n","#df4 = pd.DataFrame(y2, columns=['C2'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-SBb8dfXmyAu"},"source":["#df5 = pd.concat([df1,df3], ignore_index=True)\n","#df6 = pd.concat([df2,df4], ignore_index=True)\n","#df7 = pd.merge(df5,df6, left_index=True, right_index=True)\n","#df7"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q0WwrQP5zvhJ","executionInfo":{"status":"ok","timestamp":1597692498241,"user_tz":240,"elapsed":992,"user":{"displayName":"Benjamin Sun","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjk-X-xY0Shww4LJnya3t7kmkZ_jdFshpN7QCgydQ=s64","userId":"07662009587488003776"}},"outputId":"ad89c2b6-cba0-4d23-e245-d237367cee88","colab":{"base_uri":"https://localhost:8080/","height":161}},"source":["from imblearn.over_sampling import ADASYN\n","from collections import Counter\n","\n","\n","column_names = ['Age','Sex','L1','L2','L3','L4', 'R1','R2', 'R3','Lf1','Lf2','Lf3']\n","unique, count = np.unique(Y_train, return_counts=True)\n","y_train_value_count = {k:v for (k,v) in zip(unique,count)}\n","\n","\n","\n","ada = ADASYN()\n","X_res, Y_res = ada.fit_resample(X_train,Y_train)\n","\n","y_res = pd.DataFrame(Y_res, columns=['Diagnosis'])\n","x_res = pd.DataFrame(X_res, columns=column_names)\n","\n","\n","unique, count = np.unique(Y_res, return_counts=True)\n","y_train_value_count = {k:v for (k,v) in zip(unique,count)}\n","y_train_value_count\n","\n","res_data = pd.merge(y_res,x_res, left_index=True,right_index=True)\n","\n","def edit_sex(x):\n","  if x >0.5:\n","    x = 1\n","    return x\n","  elif x<0.5:\n","    x=0\n","    return x\n","\n","res_data['Sex'] = res_data['Sex'].apply(edit_sex)\n","y_train_value_count"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n","  y = column_or_1d(y, warn=True)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n","  warnings.warn(msg, category=FutureWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["{0: 20776, 1: 20449, 2: 20667}"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"yojj5GAD-DYI"},"source":["\n","res_data.to_csv('training4.csv')\n","res_data.to_csv('download')\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ADqkInAPATPA"},"source":["test_data = pd.merge(Y_test,X_test,left_index=True,right_index=True)\n","test_data.to_csv('test.csv')\n","test_data.to_csv('download')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qOOieyQXAVHI"},"source":["test_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e4HGSpRLuu2Z"},"source":["\n","\n","#Resampling each binary dataset using SMOTE oversampling\n","#oversample = SMOTE()\n","#X1, y1 = oversample.fit_resample(X_train, y1_new.values.ravel())\n","#X2, y2 = oversample.fit_resample(X_train, y2_new.values.ravel())\n","#X3, y3 = oversample.fit_resample(X_train, y3_new.values.ravel())\n","\n","sort_y1 = []\n","sort_x1 = []\n","sort_y2 =[]\n","sort_x2 = []\n","sort_y3 = []\n","sort_x3 = []\n","\n","#Creating new lists consisting of the newly resampled positive class samples (i.e binary lists are 1's and 0's. The following loops create a new list of just the rows which have a \"1\" diagnosis)\n","#The new arrays are then converted to dataframes with the appropriate column names to be recombined later\n","for i in range(len(y1)):\n","  if y1[i] ==1:\n","    sort_y1.append(y1[i])\n","    sort_x1.append(X1[i])\n","\n","dfy1 = pd.DataFrame(sort_y1, columns=['Diagnosis'])\n","dfx1 = pd.DataFrame(sort_x1,columns = column_names)\n","\n","for i in range(len(y2)):\n","  if y2[i] ==1:\n","    sort_y2.append(y2[i])\n","    sort_x2.append(X2[i])\n","\n","\n","#Trim the list of osteoporotic diagnoses since it was like 500 examples longer\n","sort_y2 = sort_y2[:3494]\n","sort_x2 = sort_x2[:3494]\n","dfy2 = pd.DataFrame(sort_y2, columns=['Diagnosis'])\n","dfx2 = pd.DataFrame(sort_x2,columns = column_names)\n","\n","#The replace function is used to convert the postive class \"1\" back to the original class (i.e \"2\" for osteoporotic BMD)\n","dfy2 = dfy2.replace(1,2)\n","\n","\n","for i in range(len(y3)):\n","  if y3[i] ==1:\n","    sort_y3.append(y3[i])\n","    sort_x3.append(X3[i])\n","\n","dfy3 = pd.DataFrame(sort_y3, columns=['Diagnosis'])\n","dfx3 = pd.DataFrame(sort_x3,columns = column_names)\n","dfy3 = dfy3.replace(1,0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DzwP3uLu9pmr"},"source":["#Dataframes are concatenated and merged to reform a new dataset of the resampled data and run through the original neural network\n","\n","dfy = pd.concat([dfy1,dfy2], ignore_index=True)\n","dfy = pd.concat([dfy,dfy3], ignore_index=True)\n","\n","dfx = pd.concat([dfx1,dfx2], ignore_index=True)\n","dfx = pd.concat([dfx,dfx3], ignore_index=True)\n","\n","res_data = pd.merge(dfy,dfx,left_index =True, right_index=True)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ylALCLw8g9mA"},"source":["res_data = res_data.reindex(np.random.permutation(res_data.index))\n","res_data.to_csv('training2.csv')\n","res_data.to_csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_rUCJZevDJvz","outputId":"d3fc8266-ef0f-4dcd-bbe9-b85d77b27579","colab":{"base_uri":"https://localhost:8080/","height":428}},"source":["#Read data with pandas\n","from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import random\n","import pathlib\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import OneHotEncoder\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","column_names = ['Diagnosis','Age','Sex','L1','L2','L3','L4', 'R1','R2', 'R3','Lf1','Lf2','Lf3']\n","\n","#Read files using pandas\n","#raw_dataset = pd.read_csv(\"training.csv\", names=column_names, na_values = \"?\", comment='\\t', sep=\",\", skipinitialspace=True)\n","train_dataset = res_data.copy()\n","train_dataset = train_dataset.reindex(np.random.permutation(train_dataset.index))  ##Shuffle examples\n","\n","raw_dataset = pd.read_csv(\"test.csv\", names=column_names, na_values = \"?\", comment='\\t', sep=\",\", skipinitialspace=True)\n","test_dataset = raw_dataset.copy()\n","\n","#View last 5 rows\n","test_dataset.tail()\n","train_dataset.tail()\n","\n","#Generate statistics of dataset\n","train_stats = train_dataset.describe()\n","#Grab data from coloumn: diagnosis\n","train_stats.pop(\"Diagnosis\")\n","train_stats = train_stats.transpose()\n","train_stats\n","\n","#Assigning diagnosis datasets\n","train_labels = train_dataset.pop('Diagnosis')\n","test_labels = test_dataset.pop('Diagnosis')\n","\n","#Normalize data\n","def norm(x):\n","  return (x - train_stats['mean']) / train_stats['std']\n","normed_train_data = norm(train_dataset)\n","normed_test_data = norm(test_dataset)\n","        \n","#Create sequential model with dense layers\n","def build_model():\n","  model = keras.Sequential([\n","    layers.Dense(64, activation=tf.nn.relu, input_shape=[len(train_dataset.keys())]),\n","    layers.Dense(64, activation=tf.nn.relu),\n","    layers.Dense(3, activation = 'softmax')\n","  ])\n","  optimizer = tf.keras.optimizers.Adam(learning_rate=3)\n","  model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy', 'mean_squared_error'])\n","  return model\n","\n","model = build_model()\n","model.summary()\n","\n","EPOCHS = 100\n","\n","#Train model\n","history = model.fit(normed_train_data, train_labels, batch_size = 50, \n","                    epochs=EPOCHS, validation_split = 0.2)\n","\n","hist = pd.DataFrame(history.history)\n","hist['epoch'] = history.epoch\n","hist.tail()\n","\n","loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=0, batch_size = 50)\n","print(\"\\nTesting set Mean Abs Error: {:5.2f} Diagnosis\".format(mae))\n","\n","test_predictions = [int(i) for i in np.round(model.predict(normed_test_data))]\n","\n","Comparison = test_labels.to_frame()\n","Comparison['Prediction'] = test_predictions\n","print(Comparison)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense (Dense)                (None, 64)                832       \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 64)                4160      \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 3)                 195       \n","=================================================================\n","Total params: 5,187\n","Trainable params: 5,187\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/100\n","991/991 [==============================] - 2s 2ms/step - loss: 1.3357 - accuracy: 0.3348 - mean_squared_error: 1.3357 - val_loss: 1.3260 - val_accuracy: 0.3306 - val_mean_squared_error: 1.3260\n","Epoch 2/100\n","991/991 [==============================] - 1s 1ms/step - loss: 1.3359 - accuracy: 0.3347 - mean_squared_error: 1.3359 - val_loss: 1.3260 - val_accuracy: 0.3306 - val_mean_squared_error: 1.3260\n","Epoch 3/100\n","991/991 [==============================] - 1s 1ms/step - loss: 1.3359 - accuracy: 0.3347 - mean_squared_error: 1.3359 - val_loss: 1.3260 - val_accuracy: 0.3306 - val_mean_squared_error: 1.3260\n","Epoch 4/100\n","671/991 [===================>..........] - ETA: 0s - loss: 1.3358 - accuracy: 0.3349 - mean_squared_error: 1.3358"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KIL346AhDmxv"},"source":["!pip install -q pycm\n","import pycm\n","from pycm import *\n","cm = ConfusionMatrix(actual_vector=test_labels.to_numpy(), predict_vector=test_predictions)\n","print(cm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OJmWVfktDpGh"},"source":["import matplotlib.pyplot as plt\n","\n","# Plot training & validation accuracy values\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Model accuracy', fontsize = 16)\n","plt.ylabel('Accuracy', fontsize = 16)\n","plt.xlabel('Epoch', fontsize = 16)\n","plt.legend(['Train', 'Test'], loc='upper left', fontsize = 12)\n","plt.show()\n","\n","# Plot training & validation loss values\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss', fontsize = 16)\n","plt.ylabel('Loss', fontsize = 16)\n","plt.xlabel('Epoch', fontsize = 16)\n","plt.legend(['Train', 'Test'], loc='upper left', fontsize = 12)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5_cN5RFr_bCi"},"source":["#Ignore this cell\n","\n","\n","#unique, count = np.unique(y3, return_counts= True)\n","#value_count = {k:v for (k,v) in zip(unique, count)}\n","\n","\n","#x1_scaled = preprocessing.scale(X1,axis=1)\n","#x2_scaled = preprocessing.scale(X2,axis=1)\n","#x3_scaled = preprocessing.scale(X3,axis=1)\n","#X_train = preprocessing.scale(X_train, axis=1)\n","#X_test = preprocessing.scale(X_test, axis=1)\n","\n","\n","\n","\n","\n","#y1_res = pd.DataFrame(y1,columns=['Diagnosis'])\n","#y2_res = pd.DataFrame(y2, columns=['Diagnosis'])\n","#y3_res = pd.DataFrame(y3, columns=['Diagnosis'])\n","#x1_res = pd.DataFrame(x1_scaled)\n","#x2_res = pd.DataFrame(x2_scaled)\n","#x3_res = pd.DataFrame(x3_scaled)\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QEZtif086-rt"},"source":["#X_test = pd.DataFrame(X_test)\n","#test_normal = pd.DataFrame(y3_test_new,columns=['Diagnosis'])\n","#test_low = pd.DataFrame(y1_test_new,columns=['Diagnosis'])\n","#test_os = pd.DataFrame(y2_test_new,columns=['Diagnosis'])\n","\n","\n","#train_dataset_0 = pd.merge(y3_res,x3_res, right_index=True,left_index=True)\n","#train_dataset_1 = pd.merge(y1_res,x1_res, right_index=True,left_index=True)\n","#train_dataset_2 = pd.merge(y2_res,x2_res, right_index=True,left_index=True)\n","\n","#test_dataset_normal = pd.merge(test_normal,X_test, right_index=True, left_index=True)\n","#test_dataset_low = pd.merge(test_low, X_test, right_index=True, left_index=True)\n","#test_dataset_os = pd.merge(test_os, X_test, right_index=True, left_index=True)"],"execution_count":null,"outputs":[]}]}